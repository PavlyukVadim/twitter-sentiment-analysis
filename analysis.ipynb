{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Using findspark to find automatically the spark folder\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuations from the sentence\n",
    "def remove_punctuation(sentence):\n",
    "    punctuations = list(string.punctuation)\n",
    "    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', 'â€“']\n",
    "    punctuations += extra_punctuations\n",
    "    filtered = [w for w in sentence.lower() if w not in punctuations]\n",
    "    return (\"\".join(filtered)).split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Calculate term frequencyâ€“inverse document frequency for reflecting importance of words in Tweet.\n",
    "# :param data_rdd: input data rdd\n",
    "# :return: transformed dataframe\n",
    "\n",
    "def tf_idf(data_rdd):\n",
    "    data_rdd_df = data_rdd.toDF()\n",
    "    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n",
    "    tf_data = hashing_tf.transform(data_rdd_df)\n",
    "\n",
    "    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n",
    "    tf_idf_data = idf_data.transform(tf_data)\n",
    "    return tf_idf_data.select(['label', 'words', 'features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Apply Naive Bayes Classifier to test data for predicting sentiment of Tweets.\n",
    "# :param training_df: Trained labelled data\n",
    "# :param testing_df: Test data\n",
    "# :return: transformed dataframe of predicted labels for tweets\n",
    "\n",
    "def naive_bayes_classifier(training_df, testing_df):\n",
    "    nb = NaiveBayes()\n",
    "    model = nb.fit(training_df)\n",
    "\n",
    "    return model.transform(testing_df).select(['label', 'words', 'prediction'])\n",
    "\n",
    "\n",
    "# Calculate accuracy of model against actual data\n",
    "def calculate_accuracy(result_df):\n",
    "    return 1.0 * result_df.filter(result_df.label == result_df.prediction).count() / result_df.count()\n",
    "\n",
    "\n",
    "# Generate Confusion Matrix for showing the performance of algorithm.\n",
    "# :param result_df: Dataframe returned from the model\n",
    "# :return: pandas dataframe\n",
    "def confusion_matrix(result_df):\n",
    "    true_positives = result_df.filter((result_df.label == 1.0) & (result_df.prediction == 1.0)).count()\n",
    "    true_negatives = result_df.filter((result_df.label == 0.0) & (result_df.prediction == 0.0)).count()\n",
    "    false_positives = result_df.filter((result_df.label == 0.0) & (result_df.prediction == 1.0)).count()\n",
    "    false_negatives = result_df.filter((result_df.label == 1.0) & (result_df.prediction == 0.0)).count()\n",
    " \n",
    "    print('true_positives', true_positives)\n",
    "    print('true_negatives', true_negatives)\n",
    "    print('false_positives', false_positives)\n",
    "    print('false_negatives', false_negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "def get_rdd_form_text(file_path):\n",
    "    data = sc.textFile(file_path)\n",
    "    col_rdd = data.map(lambda x: (x.split('\\t')[0], x[-1]))\n",
    "    punctuation_removed_rdd = col_rdd.map(lambda x: (remove_punctuation(x[0]), float(x[1])))\n",
    "    return punctuation_removed_rdd\n",
    "\n",
    "\n",
    "def stop_words_remover(stopWords, data_df):\n",
    "    remover = StopWordsRemover(inputCol = 'text', outputCol = 'words', stopWords = stopWords)\n",
    "    return remover.transform(data_df).select(['label', 'words'])\n",
    "\n",
    "\n",
    "def get_en_mixed_data():\n",
    "    punctuation_removed_rdd = get_rdd_form_text('data/data.txt')\n",
    "    stopWords = stopwords.words('english')\n",
    "    data_df = sqlContext.createDataFrame(punctuation_removed_rdd, ['text', 'label'])\n",
    "    data_df = stop_words_remover(stopWords = stopWords, data_df = data_df)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_uk_mixed_data():\n",
    "    sqlContext = SQLContext(sc)\n",
    "    path = 'data/data.json'\n",
    "    tweetsDF = sqlContext.read.json(path)\n",
    "    data = tweetsDF.select('isPositive', 'message')\n",
    "    filteredData = data.rdd.filter(lambda s: s.message)\n",
    "    mappedData = filteredData.map(lambda s: (float(1 if s.isPositive == True else 0), remove_punctuation(s.message)))\n",
    "\n",
    "    data_df = sqlContext.createDataFrame(mappedData, ['label', 'text'])\n",
    "    customWords = ['Ð¿Ð¾Ð³Ð°Ð½Ð¾', 'Ð¿Ð¾Ð³Ð°Ð½Ð¸Ð¹', 'Ð´Ð¾Ð±Ñ€Ðµ', 'Ð´Ð¾Ð±Ñ€Ð¸Ð¹']\n",
    "    stopWords = get_stop_words('ukrainian') + customWords\n",
    "    data_df = stop_words_remover(stopWords = stopWords, data_df = data_df)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_uk_twitter_data():\n",
    "    sqlContext = SQLContext(sc)\n",
    "    path = 'data/twitter-data-3.json'\n",
    "    tweetsDF = sqlContext.read.json(path)\n",
    "    data = tweetsDF.select('isPositive', 'text')\n",
    "    filteredData = data.rdd.filter(lambda s: s.text)\n",
    "    mappedData = filteredData.map(lambda s: (float(1 if s.isPositive == True else 0), remove_punctuation(s.text)))\n",
    "\n",
    "    data_df = sqlContext.createDataFrame(mappedData, ['label', 'text'])\n",
    "    customWords = ['Ð´Ð¾Ð±Ñ€Ðµ', 'Ð´Ð¾Ð±Ñ€Ð¸Ð¹', 'ðŸ˜Š', 'ðŸ˜˜', 'Ð¿Ð¾Ð³Ð°Ð½Ð¾', 'Ð¿Ð¾Ð³Ð°Ð½Ð¸Ð¹',  'ðŸ˜­', 'ðŸ˜¡']\n",
    "    stopWords = get_stop_words('ukrainian') + customWords\n",
    "    data_df = stop_words_remover(stopWords = stopWords, data_df = data_df)\n",
    "    return data_df\n",
    "\n",
    "# def test_data():\n",
    "#     data = sc.textFile(\"data/test.txt\")\n",
    "#     col_rdd = data.map(lambda x: (x.split('\\t')[0], x[-1]))\n",
    "#     punctuation_removed_rdd = col_rdd.map(lambda x: (float(x[1]), remove_punctuation(x[0])))\n",
    "#     data_df = sqlContext.createDataFrame(punctuation_removed_rdd, [\"label\", 'words'])\n",
    "#     return data_df\n",
    "# test_data()\n",
    "\n",
    "\n",
    "# Display accuracy and confusion matrix for the models\n",
    "# :param predicted_df: predicted dataframe of test data\n",
    "def show_stats(predicted_df):\n",
    "    predicted_df.show(5)\n",
    "    accuracy = calculate_accuracy(predicted_df)\n",
    "    confusion_table = confusion_matrix(predicted_df)\n",
    "    print('Accuracy of the model:', round(accuracy * 100, 2))\n",
    "    print('Confusion Matrix:', confusion_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-10-5684703a0b37>:2 ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d4d214d50c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# locale = sc._jvm.java.util.Locale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-10-5684703a0b37>:2 "
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag('en-US'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "count training rdd: 2089\ncount test rdd: 903\n+-----+--------------------+----------+\n|label|               words|prediction|\n+-----+--------------------+----------+\n|  1.0|[sound, quality, ...|       1.0|\n|  1.0|[good, quality, t...|       1.0|\n|  0.0|[design, odd, ear...|       1.0|\n|  1.0|[highly, recommen...|       1.0|\n|  0.0|[advise, everyone...|       1.0|\n+-----+--------------------+----------+\nonly showing top 5 rows\n\ntrue_positives 325\ntrue_negatives 351\nfalse_positives 101\nfalse_negatives 126\nAccuracy of the model: 74.86\nConfusion Matrix: None\n"
    }
   ],
   "source": [
    "filtered_data_df = get_en_mixed_data()\n",
    "# filtered_data_df = get_uk_mixed_data()\n",
    "# filtered_data_df = get_uk_twitter_data()\n",
    "\n",
    "splitParams = [0.7, 0.3]\n",
    "training, test = filtered_data_df.rdd.randomSplit(splitParams, seed = 0)\n",
    "\n",
    "print('count training rdd:', training.count())\n",
    "print('count test rdd:', test.count())\n",
    "\n",
    "train_df = tf_idf(training)\n",
    "test_df = tf_idf(test)\n",
    "\n",
    "nb_predicted_df = naive_bayes_classifier(train_df, test_df)\n",
    "show_stats(nb_predicted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit0bb5f6c311484277905fd0791662116d",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}